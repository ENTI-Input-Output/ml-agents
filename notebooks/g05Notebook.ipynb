{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G05 ML-AGENTS FOODCOLLECTORMIX\n",
    "\n",
    "## 1- Introduction\n",
    "\n",
    "<p><span>In this notebook you can find an implementation analysis of the&nbsp;FoodCollector environment in the ML-agents toolkit. We also provide a performance analysis of the agents training using TensorBoard. </span></p>\n",
    "<p><span>In addition to what has been said, we'll issue a new agent behaviour functionality for the same environment. Detailing how the agents have been trained, how rewards are assigned, and conducting an analysis of the training performance. </span></p>\n",
    "<p><span>This project has been developed by Albert Vegara Martinez and David Sola Cansell, students of the Bachelor's Degree of Digital Interactive Contents at ENTI-UB (University of Barcelona).</span></p>\n",
    "\n",
    "<table style=\"border:1px solid black;height: 353px; margin-left: auto; margin-right: auto; width: 708.667px;\">\n",
    "<tbody>\n",
    "<tr style=\"height: 110px;\">\n",
    "<td style=\"width: 222px; text-align: center; height: 110px;\"><span style=\"color: #000000;\"><strong>Name</strong></span></td>\n",
    "<td style=\"width: 222px; text-align: center; height: 110px;\"><strong>Email</strong></td>\n",
    "<td style=\"width: 245.667px; text-align: center; height: 110px;\"><strong>Picture</strong></td>\n",
    "</tr>\n",
    "<tr style=\"height: 219px;\">\n",
    "<td style=\"width: 222px; text-align: center; height: 219px;\">Albert Vegara</td>\n",
    "<td style=\"width: 222px; text-align: center; height: 219px;\">albertvegaramartinez@enti.cat</td>\n",
    "<td style=\"width: 245.667px; height: 219px;\"><img style=\"width: 200px; height: 200px\" src=\"Fotos/albert.png\"></td>\n",
    "</tr>\n",
    "<tr style=\"height: 216.333px;\">\n",
    "<td style=\"width: 222px; text-align: center; height: 216.333px;\">David Sol&agrave;</td>\n",
    "<td style=\"width: 222px; text-align: center; height: 216.333px;\">davidsolacansell@enti.cat</td>\n",
    "<td style=\"width: 245.667px; height: 216.333px;\"><img style=\"width: 200px; height: 200px\" src=\"Fotos/david.jpg\"></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Case Analysis\n",
    "\n",
    "<p>The currently implemented behaviour consists in a series of agents that search for food in an area with food and poison. The objective of the agents was to get food and avoid grabbing poison.<br />The modification we purposed was that the agent had to eat a ball of \"food\" first, and then eat a ball of \"bad food\" or \"poison\", in that order.</p>\n",
    "<p>The current implementation gives rewards in the following way:<br /> - +1 point for grabbing food<br /> - -1 point for grabbing poison</p>\n",
    "<p>The current states in which the agent can be is:<br /> - Normal state: the agent looks actively for food.<br /> - Satiated: when the agent grabs food, it is satiated for 2 seconds (It changes the agent material to green).<br /> - Poisoned: when the agent grabs bad food, it is poisoned for 2 seconds (It changes the agent material to red).<br /> - Frozen: the agents have a \"laser\" that is shot in front of them, when this laser hits another agent, that agent is frozen for 4 seconds.</p>\n",
    "<p>This training is implemented by letting the agents detect: food, bad food, other agents, walls, and frozen agents.<br />In the begining, the agents free roam and get balls randomly, after some time, the agents learn to redirect themselves towards good food balls, avoinding poison balls, thus making the most efficient path through the balls.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Performance Analysis\n",
    "\n",
    "<p>The implementation of the default behaviour consists of an agent looking for food (green spheres) and avoiding poison (red spheres).</p>\n",
    "<p>The critical parameters to make it work are: the tag of the food object (food or poison), the agent awareness (vision rays, vision cone, etc) and the agent movement parameters (movement speed, turn speed, acceleration, etc).</p>\n",
    "<p>In the default implementation, the agents roam around the area and pick up most of the food they find in the process. In order to do this, the agent uses a series of rays that scan the environment and search for food, poison, and other agents. If the agent's rays intersect a food element, the agent tries to stir in its direction and aim for the food, adapting to its trajectory.</p>\n",
    "<p>This works because it's a super simple reward system, there are only two possible outcomes: you picked the right food or you picked the wrong food. This simplicity, based with a very indicative reward system, makes the agent know immediatly if what he did is correct or not.</p>\n",
    "<p>In our approach, we tried to add a bit more complexity to the system, as explained in the next point.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto00.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- New Case Proposal\n",
    "\n",
    "<p>The modification we tried to implement was that the agent had to eat a ball of \"food\" first, and then eat a ball of \"bad food\" or \"poison\", in that order.</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p>We will only provide 1 graph with the duration of the exercise, because our challenge was not a limited time task, but a continuous task that kept playing. This makes that the duration graph always shows 1000 as value, because it is the duration of the max steps limit.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto2.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>First Attempt:</strong><br />What we first tried was giving the agent points in the following way:</p>\n",
    "<p>If the agent did not have a green ball:<br /> - +1 point if it grabs a green ball<br /> - -1 point if it grabs a red ball<br /> If the agent had a green ball:<br /> - +1 point if it grabs a red ball<br /> - -1 point if it grabs another green ball</p>\n",
    "<p>We trained the agents for 1 million iterations.<br />This made the agent extremely unefficient because it learnt that red balls or green balls could mean bad rewards, and it was \"scared\" of grabbing them from then on, so the agent was mostly just standing still.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto1.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Second Attempt:</strong><br />In this attempt, we used the same reward structure, but we left the agent training for 2 million iterations, because we thought that maybe the agent needed more time to learn.<br />The results were mostly the same as the first attempt, but in a longer extent of time.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto3.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Third Attempt:</strong><br />In this attempt, we used the same reward structure, but we tried a different approach to the detection of the balls, as suggested by Joan. What we did was modify the detectable tags of the agents to make the agent see the green and red balls only when it should grab them.<br />We trained the agents for 1 million iterations.<br />The results were similar to the previous attempts, because oftentimes the agents would grab a ball without seeing it, thus becoming \"scared\" of moving to avoid grabbing balls.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto4.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Fourth Attempt:</strong><br />In this attempt, we did the same than third attempt, but left the agents training for 2 million iterations.<br />As happened in attempts 1 and 2, the results were the same, and iterations did not matter to the final result.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto5.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Fifth Attempt:</strong><br />In this attemps, we changed the way in which we give reward:</p>\n",
    "<p>If the agent did not have a green ball:<br /> - +1 point if it grabs a green ball<br /> If the agent did have a green ball:<br /> - +1 point if it grabs a red ball</p>\n",
    "\n",
    "<p>We removed the negative reward for grabbing the balls in the wrong order, because we wanted to avoid the agent being \"scared\" to grab a certain type of ball.<br />We maintaned the modified sensors.<br />The results were very inconsistent, even though we can see a clear improvement in the performance, but, as the agent did not \"see\" all the balls at once, some times it grabbed a wrong ball unwillingly, and dropped performance.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto6.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Sixth Attempt:</strong><br />For this attempt, we re-introduced the reward system of the first attempt, and we re-introduced the agent freezing mechanic, adding a +0.5 reward every time the agent froze another agent.<br />We trained the model for 850.000 iterations.<br />This caused a weird behaviour in the agents, because they completely ignored the balls in fear of being penalized and only tried to freeze other agents.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto7.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Seventh Attempt:</strong><br />For this attempt, we removed the penalizations again and we changed the AddReward method for SetReward, and we removed the reward for freezing other agents.<br />We trained the agents for 1 milion iterations.<br />This meant an immediate improvement in the behaviour of the agents, now the agents were going for the balls instead of the other agents, and it meant an overall better performance.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto77.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Eighth Attempt:</strong><br />For this attempt, we added rigidbodies to the food.<br />We trained the agent for 2 million iterations.<br />In this attempt, we had a big problem, at one point, the food was stuck against a wall and the agents seemed to be unable to find the food. Thus, the agents did not learn good enough and the performance was overall low.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto8.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Ninth Attempt:</strong><br />For this attempt, we added to the balls a behaviour that made them reset every time they touched a ball, which made them unable to be stuck against a wall.<br />For the agents, we added more detection rays to each agent to improve their \"sight\".<br />We trained the agents for 2 million iterations.<br />The result was a vast improvement in the performance of the agents, now they behaved good enough. Even though they still missed balls or picked them in a bad order sometimes.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto9.png\">\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><strong>Tenth Attempt:</strong><br />For this attempt, we changed the SetReward method for AddReward again, to see if it would improve the agents behaviour.<br />We trained the agents for 2 million iterations.<br />The result was a downgrade in the performance of the agents. <br />We don't understand why, but the agents don't seem to like the AddReward method when having to do a series of chained actions.</p>\n",
    "\n",
    "<img style=\"width: 700px; height: 300px\" src=\"Fotos/foto10.png\">\n",
    "\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p>We've observed that the peak performance phase of the agents, where the highest cumulative reward values are reached, is around the 150,000 training steps. Despite this, we believe that this being a complex behaviour, 150,000 steps are too short of a sample to obtain significant results.</p>\n",
    "\n",
    "<hr />\n",
    "\n",
    "<p><span style=\"color: #3366ff;\"><strong>To execute the program with our most efficient approach, add the FoodCollector.nn file found in FoodCollector/TFModels/FINAL_MODEL to each agent of the scene and play the scene.</strong></span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Team\n",
    "\n",
    "<table style=\"border:1px solid black;height: 353px; margin-left: auto; margin-right: auto; width: 708.667px;\">\n",
    "<tbody>\n",
    "<tr style=\"height: 110px;\">\n",
    "<td style=\"width: 222px; text-align: center; height: 110px;\"><span style=\"color: #000000;\"><strong>Name</strong></span></td>\n",
    "<td style=\"width: 222px; text-align: center; height: 110px;\"><strong>Email</strong></td>\n",
    "<td style=\"width: 245.667px; text-align: center; height: 110px;\"><strong>Picture</strong></td>\n",
    "</tr>\n",
    "<tr style=\"height: 219px;\">\n",
    "<td style=\"width: 222px; text-align: center; height: 219px;\">Albert Vegara</td>\n",
    "<td style=\"width: 222px; text-align: center; height: 219px;\">albertvegaramartinez@enti.cat</td>\n",
    "<td style=\"width: 245.667px; height: 219px;\"><img style=\"width: 200px; height: 200px\" src=\"Fotos/albert.png\"></td>\n",
    "</tr>\n",
    "<tr style=\"height: 216.333px;\">\n",
    "<td style=\"width: 222px; text-align: center; height: 216.333px;\">David Sol&agrave;</td>\n",
    "<td style=\"width: 222px; text-align: center; height: 216.333px;\">davidsolacansell@enti.cat</td>\n",
    "<td style=\"width: 245.667px; height: 216.333px;\"><img style=\"width: 200px; height: 200px\" src=\"Fotos/david.jpg\"></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
