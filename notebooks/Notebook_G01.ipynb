{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Output. Delivery One\n",
    "\n",
    "<img src=\"../docs/images/image-banner.png\" align=\"middle\" width=\"3000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G01: Victor Armisen and David Recuenco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "The purpose of this notebook is explaining our work with the environment \"Tennis\" from the ml-agents examples.\n",
    "This example, as its name says, simulates a tennis match between two agents which follows the real tennis rules.\n",
    "What we wanted to do with the example was using only an agent and:\n",
    "* Make the agent play paddle alone and following the game's rules: the ball has to touch the ground once before being hit and if the ball touches the ground two times in a row without touching the front wall, the point is lost.\n",
    "* Make the agent do keepy-ups not letting the ball touching the ground. \n",
    "* The same keepy-ups as above but with a force and racket rotations, making it harder for the agent to keep the ball in the air."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The team\n",
    "Name | Enti email | Picture\n",
    "--- | --- | ---\n",
    "Victor Armisen | victorarmisencapo@enti.cat | placeholder picture\n",
    "David Recuenco | davidrecuencooliver@enti.cat | <img src=\"../docs/images/DRO.png\" width=\"100\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Case Analysis\n",
    "The example is managed by 3 scripts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **TennisArea**\n",
    "\n",
    "Manages the ball's physics and has the function to reset the match, spawning the ball in a random side of the court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    public void MatchReset()\n",
    "    {\n",
    "        var ballOut = Random.Range(6f, 8f);\n",
    "        var flip = Random.Range(0, 2);\n",
    "        if (flip == 0)\n",
    "        {\n",
    "            ball.transform.position = new Vector3(-ballOut, 6f, 0f) + transform.position;\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            ball.transform.position = new Vector3(ballOut, 6f, 0f) + transform.position;\n",
    "        }\n",
    "        m_BallRb.velocity = new Vector3(0f, 0f, 0f);\n",
    "        ball.transform.localScale = new Vector3(.5f, .5f, .5f);\n",
    "        ball.GetComponent<HitWall>().lastAgentHit = -1;\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **TennisAgent**\n",
    "\n",
    "Obviously, manages the agent which in this case is the racket. The script has the heuristic, movement and reset.\n",
    "In the agent we slightly change its properties in the case of Keep Up to simulate the touches.\n",
    "In all cases, we use the inputs of the vectorActions to change the speeds and rotations of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tennis Agent",
    "var moveX = Mathf.Clamp(vectorAction[0], -1f, 1f) * m_InvertMult;\n",
    "    var moveY = Mathf.Clamp(vectorAction[1], -1f, 1f);\n",
    "    var rotate = Mathf.Clamp(vectorAction[2], -1f, 1f) * m_InvertMult;\n",
    "    if (moveY > 0.5 && transform.position.y - transform.parent.transform.position.y < -1.5f)\n",
    "    {\n",
    "        m_AgentRb.velocity = new Vector3(m_AgentRb.velocity.x, 7f, 0f);\n",
    "    }\n",
    "    m_AgentRb.velocity = new Vector3(moveX * 30f, m_AgentRb.velocity.y, 0f);\n",
    "    m_AgentRb.transform.rotation = Quaternion.Euler(0f, -180f, 55f * rotate + m_InvertMult * 90f);\n",
    "    if (invertX && transform.position.x - transform.parent.transform.position.x < -m_InvertMult ||\n",
    "        !invertX && transform.position.x - transform.parent.transform.position.x > -m_InvertMult)\n",
    "    {\n",
    "        transform.position = new Vector3(-m_InvertMult + transform.parent.transform.position.x,\n",
    "            transform.position.y,\n",
    "            transform.position.z);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **HitWall**\n",
    "\n",
    ".\n",
    "The example checks the collision of the ball on both sides of the court to determine which player has won the point. The winning player received a positive reward and the loser a negative one, the score was added to the canvas and the scene was reset. It also checks the double bounce of the ball on each of the tracks with the variable lastFloorHit\n", 
    "and determines the service served with net. All this to verify that victory condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            else if (collision.gameObject.name == \"wallB\")\n",
    "            {\n",
    "                // Agent B hits into wall or agent A hit a winner\n",
    "                if (lastAgentHit == 1 || lastFloorHit == FloorHit.FloorBHit)\n",
    "                {\n",
    "                    AgentAWins();\n",
    "                }\n",
    "                // Agent A hits long\n",
    "                else\n",
    "                {\n",
    "                    AgentBWins();\n",
    "                }\n",
    "            }\n",
    "            else if (collision.gameObject.name == \"floorA\")\n",
    "            {\n",
    "                // Agent A hits into floor, double bounce or service\n",
    "                if (lastAgentHit == 0 || lastFloorHit == FloorHit.FloorAHit || lastFloorHit == FloorHit.Service)\n",
    "                {\n",
    "                    AgentBWins();\n",
    "                }\n",
    "                else\n",
    "                {\n",
    "                    lastFloorHit = FloorHit.FloorAHit;\n",
    "                    //successful serve\n",
    "                    if (!net)\n",
    "                    {\n",
    "                        net = true;\n",
    "                    }\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards:\n",
    "Rewards, as mentioned above, are awarded when the collision is detected on the corresponding track side.\n",
    "The possibilities are calculated for each agent and the reward is acquired with the SetReward () function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
     "void AgentAWins()\n",
   " {\n",
       " m_AgentA.SetReward(1);\n",
        "m_AgentB.SetReward(-1);\n",
       " m_AgentA.score += 1;\n",
        "Reset();\n",
   " }\n",
    "void AgentBWins()\n",
    "{\n",
       "m_AgentA.SetReward(-1);\n",
       " m_AgentB.SetReward(1);\n",
       " m_AgentB.score += 1;\n",
       " Reset();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States:\n",
    "To control all of the above, the example uses a few simple states to determine each event that occurs during the game. \n",
    "It simply detects service, when the ball hits each side of each agent's court and another would detect if there is no direct bounce from the ball.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   "public enum FloorHit\n",
    "{\n",
        "Service,\n",
        "FloorHitUnset,\n",
        "FloorAHit,\n",
        "FloorBHit\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:\n",
    "We do learning checks around a 100K Steps. From then on, we consider that, if we see correct results, the model is worth it.\n",
    "We check these results through the variables used and the graphs that we obtain locally with Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis\n",
    "Explicaci√≥n\n",
    "\n",
     "Once the analysis is performed, the example will work quickly. We consider the following.\n",
     "The example represents an ELO variable for each player to know the degree of success they have in their actions when playing against the enemy bot.\n",
     "Then, in tensorFlow, we have the cumulative values of the player that indicate his learning progress and a more specific analysis of him.\n",
     "Finally, it shows us the graph of Episode.\n",
     "By testing, we are clear that you cannot miss the reward of simply having the ball touch the racket. Without this positive reward, we cannot derive other more specific events that will help us carry out our cases.\n",
     "As negative rewards, they would be the collisions with the ground, which allow us to drift to walls or other objects that we could insert.\n",
     "In this perform analysis, we have been able to see the agent's learning curve. It has allowed us to see the phases you go through and the time / steps you need to learn to adequately and perfectly perform these actions performed by the trained brain .nn.\n",
     "The progression we are talking about would be: 1) begin to perform the basic actions in realizations at changes in speed and rotations 2) Little by little, it will hold and be more stable and minimizing the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. New case proposal\n",
    "\n",
    "As mentioned in the introduction, we have three different cases to train the agent with.\n",
    "For making this possible we had to remove one of the scene's agents and adapt the scripts to one agent only since they were made for two.\n",
    "\n",
    "The spawn of the ball had to be modified since it was randomly spawned to both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        var ballOut = Random.Range(-6f, -8f); // distancia en x\n",
    "        ball.transform.position = new Vector3(ballOut, 8f, 0f) + transform.position;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paddle:\n",
    "\n",
    "<img src=\"../docs/images/Paddle.png\" align=\"middle\"/>\n",
    "\n",
    "For the Paddle case all was focused on the collisions the ball made. For this, an enum was used as for a status machine in order to check what it colided with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    void OnCollisionEnter(Collision collision) {\n",
    "        switch (state) {\n",
    "            case Status.Floor:\n",
    "                if (collision.gameObject.name == \"Agent\") {\n",
    "                    state = Status.Agent;\n",
    "                } \n",
    "                else Death();\n",
    "                break;\n",
    "\n",
    "            case Status.Agent:\n",
    "                if (collision.gameObject.name == \"WallFront\") {\n",
    "                    if (!firstGame) {\n",
    "                        currentTouches++;\n",
    "                        GivePositiveReward();\n",
    "                    }\n",
    "                    else {\n",
    "                        firstGame = false;\n",
    "                        GivePositiveReward_Less();\n",
    "                    }\n",
    "                    state = Status.Wall;\n",
    "                }\n",
    "                else Death();\n",
    "                break;\n",
    "\n",
    "            case Status.Wall:\n",
    "                if (collision.gameObject.name == \"Floor\") state = Status.Floor;\n",
    "                else Death();\n",
    "                break;\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of reward given to the agent:\n",
    "* **GivePositiveReward()** Gives a reward of value 1. Used for the normal touches.\n",
    "* **GivePositiveReward_Less()** Gives a reward of value 0.5. Used for the first successful touch since the next ones are the ones that count.\n",
    "\n",
    "As for the results, the agent's learning is slow at first but as it's swon in the graphs, it learns exponentially\n",
    "<img src=\"../docs/images/Paddle_ELO.png\" align=\"middle\"/>\n",
    "<img src=\"../docs/images/Paddle_AR.png\" align=\"middle\"/>\n",
    "<img src=\"../docs/images/Paddle_EL.png\" align=\"middle\"/>\n",
    "\n",
    "One trick used to help out the agent to learn faster got nothing to do with the rewards: I changed the height position of the spawn of the ball so the agent does not need to wait for the ball to fall. The ball spawns close to the agent so he can hit the ball and let it enough space to hit the wall and then the floor without letting the agent hit the ball in the middle that easily. Another trick used to help out the agent was keeping the rewards constant to it gets used to the training without changes while learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keepy-ups simple: No forces and no racket rotations\n",
    "<img src=\"../docs/images/KeepsUps.png\" align=\"middle\"/>\n",
    "\n",
    "In the first case of Keepy-ups, we have a first case without rotation in which the racket has to learn to approach the instanced ball in a random way and keep the touches up.\n",
    "In the second case, the racket has to learn to control the rotation of the racket and we send the ball to different places already in XYZ. When instantiating the ball, we apply a force to it so that it does not simply fall.\n",
    "<img src=\"../docs/images/EX2_Tennis_G01.png\" align=\"middle\"/>\n",
    "The cases are intentionally made so the agent first learns to move in the environment and match the ball and then learn more complex situations with randomized force and control of the ball with the rotations of the racket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Code Keepy-ups",
    "//EX3: Wind and Rotation",
    "Vector3 dir = ball.transform.position - transform.position;\n",
    "dir.Normalize();\n",
    "\n",
    "distance = ball.transform.position.x - transform.position.x;\n",
     "distance = Mathf.Abs(distance);\n",
     "if (distance < 2.0f)\n",
     "{\n",
         "AddReward(1);\n",
     "}\n",
     "else\n",
     "{\n",
         "AddReward(-1);\n",
     "}\n",
    "m_AgentRb.velocity = new Vector3(moveX * dir.x * 30.0f, m_AgentRb.velocity.y, 0f); //We move in XY.\n",
    "\n",
    "//EX3: Wind and Rotation\n",
    "m_AgentRb.velocity = new Vector3(moveX * dir.x * magnitude, m_AgentRb.velocity.y, moveX * dir.z * magnitude); //We move in XYZ.\n",
    "//We removed the Invert boolean from the original example since we only use a racket and no sides. The racket learns to rotate on the Z axis to control the ball.\n",
            "m_AgentRb.transform.rotation = Quaternion.Euler(-180f, -180f, 55f * rotate);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keepy-ups with a force and racket rotation control ball\n",
    "We calculate the distance between the ball and the agent to know the degree of accuracy at which it crosses the ball's path.\n",
    "<img src=\"../docs/images/EX3_Tensor.png\" align=\"middle\"/>\n",
    "\n",
    "We give positive reward if the ball touches the racket and negative if it touches the floor or invisible walls that we have simulated. The scene is only reset if it touches the ground.\n",
    "If it touches the wall, we don't reset it to help you and improve the result. Floor -1 Wall -0.5 Racket 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Awards for keepy-ups\n",
    "void OnCollisionEnter(Collision collision)\n",
    "{\n",
        "if (collision.gameObject.tag == \"iWall\")\n",
        "{\n",
           " m_Agent.AddReward(-0.5f);\n",
           " if (!EX3)\n",
           " {\n",
           "     Reset();\n",
           " } \n",
           " else\n",
         "   {\n",
           "     gameObject.GetComponent<Rigidbody>().velocity = Vector3.zero;\n",
           " }\n",        
        "}\n",
       " if (collision.gameObject.tag == \"Agent\")\n",
       " {\n",
       "     m_Agent.AddReward(2);\n",
       " }\n",
       " if(collision.gameObject.name == \"Floor\")\n",
      "  {\n",
           " m_Agent.AddReward(-1);\n",
            "Reset();\n",
        "}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
