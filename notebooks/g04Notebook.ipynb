{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÀCTICA ML-AGENTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- INTRODUCTION\n",
    "\n",
    "L'objectiu d'aquest notebook és analitzar i implementar una escena on un agents aprenen a satisfer un objectiu mitjançant un algoritme de Reinforcement Learning.\n",
    "\n",
    "Els creados d'aquesta llibreta i de l'exercici sóm: \n",
    "\n",
    "<table style=\"font-size: 15px;   border-collapse: collapse; width: 100%;\">\n",
    "  <tr style=\"  background-color: black; color: white; \">\n",
    "    <th>Name</th>\n",
    "    <th>Contact</th>\n",
    "    <th>Photo</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Àlex Gómez Cortés</td>\n",
    "    <td>alexgomezcortes@enti.cat</td>\n",
    "    <td><img src=\"images/yo.jpg\" style=\"margin-left:0px; width: 200px; height: 200px\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Àlex Michalec Canellas</td>\n",
    "    <td>alexmichaleccanellas@gmail.com</td>\n",
    "    <td><img src=\"images/0.jfif\" style=\"margin-left:0px; width: 200px; height: 200px\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- CASE ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'escena que hem analitzat per aquest exercici s'anomena Hallway. En aquesta escena l'agent ha d'aconseguir arribar a un símbol O o X de manera satisfactoria. Aquest símbol és a un lloc diferent a cada iteració.\n",
    "\n",
    "Per aconseguir aixó, a l'agent se li dónen un conjunt de recompenses i penalitzacions per a que aprengui a arribar.\n",
    "\n",
    "L'agent coneix a l'objecte que té el símbol en qüestió, pel que sap on hi és. L'agent al principi, no sap el que ha de fer, i comença a probar de moure's per l'entorn. Arriba un moment, però, que l'agent supera el número d'steps màxim i aleshores se'l penalitza si no ha assolit l'objectiu. També se'l penalitza si no arriba a l'objectiu en el temps marcat i si arriba al simbol equivocat.\n",
    "\n",
    "A la mateixa vegada, però, se'l recompensa si assoleix l'objectiu. A partir d'aquí, sap el que ha de fer, pel que a cada nova iteració veiem com va millorant i al final avança cap a l'objectiu pel camí més òptim.\n",
    "\n",
    "Aixó és en quant als rewars i a les penalitzacions, però l'script utilitzat té més funcions:\n",
    "\n",
    "Té una funció que mou l'agent en direccions que li dóna el brain:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Current Agent action (message sent from Brain).\n",
    " AgentAction m_Action;\n",
    "------------------------------------------------------\n",
    " if ((m_RequestAction) && (m_Brain != null))\n",
    "            {\n",
    "                m_RequestAction = false;\n",
    "                if (m_Action.vectorActions != null)\n",
    "                {\n",
    "                    AgentAction(m_Action.vectorActions);\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "-------------------------------------------------------------\n",
    "   public override void AgentAction(float[] vectorAction)\n",
    "    {\n",
    "        AddReward(-0.1f / maxStep);\n",
    "        MoveAgent(vectorAction);\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "També té una funció que filtra les col·lisions amb l'entorn segons tags, i dóna reward o penalitza segons amb el que xocat:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "    void OnCollisionEnter(Collision col)\n",
    "    {\n",
    "        if (col.gameObject.CompareTag(\"symbol_O_Goal\") || col.gameObject.CompareTag(\"symbol_X_Goal\"))\n",
    "        {\n",
    "            if ((m_Selection == 0 && col.gameObject.CompareTag(\"symbol_O_Goal\")) ||\n",
    "                (m_Selection == 1 && col.gameObject.CompareTag(\"symbol_X_Goal\")))\n",
    "            {\n",
    "                    SetReward(1f);\n",
    "                    StartCoroutine(GoalScoredSwapGroundMaterial(m_HallwaySettings.goalScoredMaterial, 0.5f));\n",
    "            }\n",
    "            else\n",
    "            {\n",
    "                SetReward(-0.1f);\n",
    "                StartCoroutine(GoalScoredSwapGroundMaterial(m_HallwaySettings.failMaterial, 0.5f));\n",
    "            }\n",
    "            Done();\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ademés té una funció que el reseteja, la qual es crida quan s'acaba la iteració actual de l'entrenament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En quant als estats, en aquesta escena només n'hi ha un: buscant l'objectiu. Nosaltres, al modificar l'escena, n'hi vam afegir un altre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- PERFORMANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En quant al rendiment de les dues escenes (la que ja hi havia i la nostra on vam afegir-hi coses), podem observar-hi algunes diferències:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El que ja estava fet:\n",
    "\n",
    "<img src=\"images/tensorboard.png\" style=\"margin-left:0px; width: 500px; height: 400px\">\n",
    "\n",
    "Com podem observar a la gràfica d'adalt a l'esquerra, els agents aprenen bastant ràpid, és a dir, ens sobra temps d'entrenament. I com podem observar a la gràfica d'adalt a la dreta, com aprenen molt ràpid, de seguida triguen molt poc temps en assolir l'objectiu.\n",
    "\n",
    "<img src=\"images/tensorboard1.png\" style=\"margin-left:0px; width: 500px; height: 400px\">\n",
    "\n",
    "Com podem veure a la gràfica d'adalt a l'esquerra, després d'afegir-li coses, podem assumir que ens sobra temps d'entrenament, però si li donem més temps, veiem com aprenen una mica més després de moltíssimes iteracions. A la vegada, a la gràfica d'adalt a la dreta, veiem com primer aprèn a arribar a l'objectiu molt ràpid però sense agafar l'objecte (ho expliquem al següent punt), pel que veiem un pic (on esta aprenent a realitzar tots els objectius). Però, de seguida aprèn a assolir-los tots, pel que concluïm que ens sobra temps d'entrenament."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- NEW CASE PROPOSAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Després d'analitzar l'escena comentada anteriorment, ens vam decidir per modificar-la i fer-la una mica més complexa, afegint-hi un nou estat, un nou objectiu i una nova penalització:\n",
    "\n",
    "<ul>\n",
    "  <li><p style=\" font-weight: bold\">Arribar a l'objectiu agafant abans un objecte: Li donem un reward si agafa l'objecte i només li donem reward al arribar a l'objectiu si té l'objecte, d'aquesta manera ens assegurem de que l'agafi abans d'anar a l'objectiu. Comprobem aixó afegint una mica de codi a la funció que filtra les col·lisions:</p> <br> \n",
    "      <p style=\"color: blue\">\n",
    "      if(col.gameObject.tag == \"collectible\")<br>\n",
    "        {<br>\n",
    "            hasCollectible = true;<br>\n",
    "            AddReward(0.5f);<br>\n",
    "            //StartCoroutine(GoalScoredSwapGroundMaterial(m_HallwaySettings.goalScoredMaterial, 0.5f));<br>\n",
    "            collectible.SetActive(false);<br>\n",
    "        } <br>\n",
    "      <br>\n",
    "       if(hasCollectible) <br>\n",
    "                { <br>\n",
    "                    SetReward(1f);<br>\n",
    "                    StartCoroutine(GoalScoredSwapGroundMaterial(m_HallwaySettings.goalScoredMaterial, 0.5f));<br>\n",
    "                }<br>\n",
    "      <br></p>\n",
    "    </li>\n",
    "  <li><p style=\" font-weight: bold\">Ademés, tal i com hem comentat prèviament, el penalitzem d'una nova forma: em possat \"forats\" al terra i si hi col·lisiona és penalitzat:</p> <br><p style=\"color: blue\">\n",
    "     if (col.gameObject.tag == \"hole\")<br>\n",
    "        {<br>\n",
    "            SetReward(-0.1f);<br>\n",
    "            StartCoroutine(GoalScoredSwapGroundMaterial(m_HallwaySettings.failMaterial, 0.5f));<br>\n",
    "            Done();<br>\n",
    "        }</li></p>\n",
    "</ul>\n",
    "\n",
    "El setup per executar la build i veure com lels agents entrenen, és el següent: Descomprimir el .zip i executar l'exe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- TEAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"font-size: 15px;   border-collapse: collapse; width: 100%;\">\n",
    "  <tr style=\"  background-color: black; color: white; \">\n",
    "    <th>Name</th>\n",
    "    <th>Contact</th>\n",
    "    <th>Photo</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Àlex Gómez Cortés</td>\n",
    "    <td>alexgomezcortes@enti.cat</td>\n",
    "    <td><img src=\"images/yo.jpg\" style=\"margin-left:0px; width: 200px; height: 200px\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Àlex Michalec Canellas</td>\n",
    "    <td>alexmichaleccanellas@gmail.com</td>\n",
    "    <td><img src=\"images/0.jfif\" style=\"margin-left:0px; width: 200px; height: 200px\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
