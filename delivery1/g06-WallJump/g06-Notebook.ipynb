{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G06 - Wall Jump\n",
    "---\n",
    "\n",
    "## Index\n",
    "[1.Introduction](#intro) <br>\n",
    "[2.Case analysis](#oldCase) <br>\n",
    "[3.Performance analysis](#performance) <br>\n",
    "[4.New case proposal](#newCase) <br>\n",
    "[5.Team](#team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"intro\"></a>1. Introduction\n",
    "This notebook has all the information related to <b>G06</b>'s delivery 1. Here you can find an analysis of the <b>Wall Jump</b> example of Unity's ml-agents, as well as a new case proposal of the same example, with all information needed to reproduce it by yourself.\n",
    "\n",
    "This notebook also works as a post mortem for our new proposal's implementation.\n",
    "\n",
    "|![AlexRivero](g06-img/Alex.png)|![DavidRecuero](g06-img/David.png)|\n",
    "|---|---|\n",
    "|Alex Rivero Ferr√†s|David Recuero Redrado|\n",
    "|alexriveroferras@enti.cat|davidrecueroredrado@enti.cat|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"oldCase\"></a>2. Case analysis\n",
    "\n",
    "### Rewards\n",
    "In this example there are 3 rewards given to the agent depending on its performance solving the problem:\n",
    "- <span style=\"color:green\">+1.0, if the agent reaches the goal</span> - Given to the agent in the <i>OnTriggerStay()</i> (using the collider of the goal)\n",
    "- <span style=\"color:red\">-0.0005, for every step the agent does</span> - Given to the agent in <i>MoveAgent()</i>. This reward motivates the agent to find the optimal path to the goal (using as less movements as possible)\n",
    "- <span style=\"color:red\">-1.0, if the agent or the cube falls off the platform</span> - Given to the agent in <i>AgentAction()</i>\n",
    "![MoveAgent](g06-img/Rewards.png)\n",
    "\n",
    "### Actions\n",
    "The agent can take 4 different main actions, each of them with multiple options:\n",
    "<img style=\"float: right;\" src=\"g06-img/Movement.gif\">\n",
    "- Forward movement\n",
    "    - Forward\n",
    "    - Backwards \n",
    "    - No action\n",
    "- Side movement\n",
    "    - Left\n",
    "    - Right\n",
    "    - No action\n",
    "- Rotation\n",
    "    - Left\n",
    "    - Right\n",
    "    - No action\n",
    "- Jump\n",
    "    - Jump\n",
    "    - No action\n",
    "    \n",
    "### Observations\n",
    "The agent has 2 different observations in the <i>CollectObservations()</i> function: the position of the agent and a boolean indicating if the agent is grounded (as the agent only receives the reward if it stays on the goal being grounded). It also has 14 ray casts each detecting 4 possible objects.\n",
    "\n",
    "### How it all works\n",
    "In this example the agent has to reach a green area tagged as \"goal\". The environment has 3 different states:\n",
    "- The goal accessible for the agent <b>without any wall</b> blocking it.\n",
    "- A <b>small-sized wall</b> blocking the way to the goal. In this case the agent can jump over the wall with a simple jump.\n",
    "- A <b>big-sized wall</b> blocking the way to the goal. In this case the agent has to push a cube against the wall, jump on the cube and then jump over the wall.\n",
    "\n",
    "As this example has 3 different states it would take too long to train it using the \"hard way\" (trying randomly every situation). That's why it uses curriculum learning.<br>\n",
    "Curriculum learning uses progression to train the agent. In this example, the wall scales up when the agent reaches a threshold. The agent learns to solve the 3 states progressively, making it easier and faster to train.\n",
    "\n",
    "To do so, the agent uses 2 brains and, depending on the current case, the brain is passed to the behavior parameters using the <i>ConfigureAgent()</i> function.\n",
    "\n",
    "```c#\n",
    "void ConfigureAgent(int config)\n",
    "    {\n",
    "        var localScale = wall.transform.localScale;\n",
    "        if (config == 0)\n",
    "        {\n",
    "            localScale = new Vector3(\n",
    "                localScale.x,\n",
    "                Academy.Instance.FloatProperties.GetPropertyWithDefault(\"no_wall_height\", 0),\n",
    "                localScale.z);\n",
    "            wall.transform.localScale = localScale;\n",
    "            GiveModel(\"SmallWallJump\", noWallBrain);\n",
    "        }\n",
    "        else if (config == 1)\n",
    "        {\n",
    "            localScale = new Vector3(\n",
    "                localScale.x,\n",
    "                Academy.Instance.FloatProperties.GetPropertyWithDefault(\"small_wall_height\", 4),\n",
    "                localScale.z);\n",
    "            wall.transform.localScale = localScale;\n",
    "            GiveModel(\"SmallWallJump\", smallWallBrain);\n",
    "        }\n",
    "        else\n",
    "        {\n",
    "            var min = Academy.Instance.FloatProperties.GetPropertyWithDefault(\"big_wall_min_height\", 8);\n",
    "            var max = Academy.Instance.FloatProperties.GetPropertyWithDefault(\"big_wall_max_height\", 8);\n",
    "            var height = min + Random.value * (max - min);\n",
    "            localScale = new Vector3(\n",
    "                localScale.x,\n",
    "                height,\n",
    "                localScale.z);\n",
    "            wall.transform.localScale = localScale;\n",
    "            GiveModel(\"BigWallJump\", bigWallBrain);\n",
    "        }\n",
    "    }\n",
    "```\n",
    "The <i>SmallWallJump</i> brain is used for both when there's no wall and when the wall is low enough to jump over it. The <i>BigWallJump</i> brain is used when the wall is too high and the agent needs the cube to jump over it.\n",
    "\n",
    "The variables <i>no_wall_min_height</i>, <i>small_wall_min_height</i> and <i>big_wall_min_height</i> are defined in a document called <b>wall_jump.yaml</b> (in the path <i>ml-agents\\config\\curricula</i>). In this file there's the setup for the curriculum learning:\n",
    "\n",
    "```c#\n",
    "BigWallJump:\n",
    "  measure: progress\n",
    "  thresholds: [0.1, 0.3, 0.5]\n",
    "  min_lesson_length: 100\n",
    "  signal_smoothing: true\n",
    "  parameters:\n",
    "    big_wall_min_height: [0.0, 0.922, 1.0 , 1.5]\n",
    "    big_wall_max_height: [0.922, 1.3, 1.5, 1.5]\n",
    "\n",
    "SmallWallJump:\n",
    "  measure: progress\n",
    "  thresholds: [0.1, 0.3, 0.5]\n",
    "  min_lesson_length: 100\n",
    "  signal_smoothing: true\n",
    "  parameters:\n",
    "    small_wall_height: [0.1, 0.3, 0.5, 0.9222]\n",
    "```\n",
    "\n",
    "Here's a link to the repository's docs where curriculum learning is explained: https://github.com/ENTI-Input-Output/ml-agents/blob/master/docs/Training-Curriculum-Learning.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"performance\"></a>3. Performance analysis\n",
    "To test the performance of the training we've tried to train the agents using both curriculum learning and the \"normal\" reinforcement learning. We expected to see a faster training and better results when using curriculum learning, and that's what we obtained:\n",
    "\n",
    "In the <i>small_wall_jump</i> situation, we can see there is no big difference between the curriculum learning and the usual method (the blue line shows the mean reward progression of the curriculum learning).\n",
    "![SmallWallJump](g06-img/SmallWallJump_01.png)\n",
    "\n",
    "However, in the <i>big_wall_jump</i> situation the difference is very noticeable. It took more than 8 million of iterations for the agent to achieve a mean reward of ~0.6, whilst using curriculum learning it took the exact amount of 100k iterations.\n",
    "![BigWallJump](g06-img/BigWallJump_01.png)\n",
    "\n",
    "We also tried to apply the advice that some authors give in their articles and we eliminated the negative reward for falling off the platform. Instead, we started to give a very small positive reward to the agent (+0.002). This accelerated the training, but not with the results we expected (didn't reach the expected mean reward nor the behavior). Instead of reaching the goal, the agent learned to always fall off the platform as it never reached the goal and didn't know that there were a bigger reward when reaching it.\n",
    "![PositiveReward](g06-img/Small&BigWallJump_PositiveReward.png)\n",
    "![AgentFallingOff](g06-img/AgentFallingOff.gif)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The main problem we've encountered is that we haven't been able to reproduce the same the training that comes with the ml-agents project. Even though we didn't change any value no line of code, the agent never learned to jump on the cube to jump over the wall. We've reached the conclusion that the the project doesn't come with the configuration needed to reproduce the given reuslts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"newCase\"></a>4. New case proposal\n",
    "As a new case, we first thought of increasing the vertical size of the wall and add a hole to force the agent push the cube to area below the hole and then jump through it. The first problem we found was the one mentioned above. The agent wasn't able to jump on the cube so it would never go through the hole. Instead, it just stood in front of the wall without doing anything.\n",
    "\n",
    "Another problem we faced was that the collider used to detect the hole was wider than the wall. Combined with the fact that we were using the <i>AddReward()</i> function, the agent learned to just approach the wall and jump against it so it entered the collider and got the reward. We solved it by decreasing the size of the hole's collider.\n",
    "\n",
    "As we were unable to solve the first problem, we decided to eliminate the <i>big wall jump</i> case and put the hole lower so the agent could jump through it without using the white cube. This time we obtained the expected results:\n",
    "\n",
    "![NewCaseWorking](g06-img/NewCaseWorking.gif)\n",
    "![NewCaseGraphic](g06-img/NewCaseGraphic.png)\n",
    "\n",
    "To configure this new case we did the following:\n",
    "\n",
    "1. We created a new variable for a new brain (for our case proposal)<br>\n",
    "![NewCaseVariable](g06-img/NewCaseVariable.png)\n",
    "\n",
    "\n",
    "2. Then we added our new case to the function <i>ConfigureAgent()</i><br>\n",
    "![NewCaseBrain](g06-img/NewCaseBrain.png)\n",
    "\n",
    "\n",
    "3. After that, we created a function to give a reward of 0.05 to the agent when it enters the hole. To do this, we needed to create a tag named Hole<br>\n",
    "![OnTriggerEnterHole](g06-img/OnTriggerEnterHole.png)\n",
    "\n",
    "\n",
    "4. For the agent to detect the hole, we needed to add the tag we created to the 2 Ray Perception Sensors detectable tags<br>\n",
    "![NewCaseRaySensor](g06-img/NewCaseRaySensor.png)\n",
    "\n",
    "\n",
    "5. As we just increased the number of observations, we had to adjust the vector observation size<br>\n",
    "![NewCaseObservationSize](g06-img/NewCaseObservationSize.png)\n",
    "\n",
    "\n",
    "6. To create the hole in the wall we created a new prefab using 4 cubes and a 5th object on trigger mode to put on the hole. The wall is created dynamically using a script that puts all the parts in place depending on the desired size of the hole<br>\n",
    "![NewCaseDynamicWall](g06-img/NewCaseDynamicWall.png)\n",
    "\n",
    "\n",
    "7. Finally, we eliminated the wall gameObject from the <i>WallJumpArea</i> prefab and we added our own prefab. Then added some code to instantiate it in the <i>ConfigureAgent()</i>function<br>\n",
    "![NewCaseInstantiateWall](g06-img/NewCaseInstantiateWall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"team\"></a>5. Team\n",
    "\n",
    "|![AlexRivero](g06-img/Alex.png)|![DavidRecuero](g06-img/David.png)|\n",
    "|---|---|\n",
    "|Alex Rivero Ferr√†s|David Recuero Redrado|\n",
    "|alexriveroferras@enti.cat|davidrecueroredrado@enti.cat|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
