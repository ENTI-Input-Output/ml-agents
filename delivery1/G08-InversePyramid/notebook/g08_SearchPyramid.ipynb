{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "El objetivo de este notebook es analizar el comportamiento del agente en la escena de \"Pyramids\" del ml_agents, dónde el agente debe buscar un botón por el escenario y, posteriormente, encontrar una pirámide concreta con una pieza encima de ella para recogerla. En nuestro caso vamos a añadir la feature de una vez recogida, llevarla hasta el botón.\n",
    "\n",
    "#### Equipo\n",
    "\n",
    "<img src=\"images/samuel.jpg\"/>\n",
    "Samuel Balcells - samuelbalcellsvaldivia@enti.cat\n",
    "\n",
    "<img src=\"images/sergi.png\"/>\n",
    "Sergi González - sergigonzalezppons@enti.cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Análisis del caso\n",
    "\n",
    "En este caso el agente debe buscar por el escenario, el cual está lleno de seis pirámides, un botón que al ser activado hace spawnear una pirámide extra en el mundo. Cada pirámide está formada por cubos no unidos, apilados. Una de las cuales cuenta con un cubo encima. El agente deberá empujarlos cubos que forman la pirámide que lo sostiene, para así tirarlo al suelo y cogerlo. Cuando el agente toque el cubo en cuestión, se acabará la simulación y recibirá las recompensas.\n",
    "\n",
    "La recompensa del agente va decreciendo en función de los pasos que de antes de completar el desafío. Y estas recompensas se le son dadas cuando consigue llegar al \"Goal\".\n",
    "\n",
    "Para hacer esto el switch o interruptor es el que maneja los estados del juego, pasando de inactivo a activo en función de cuando el agente colisione con él. En ese momento le da la orden al escenario de empezar a crear la pirámide extra, con el \"Goal\" encima, dando así paso a la segunda parte de la simulación. Las posiciones de estas pirámides vienen determinadas por zonas gestionadas por el switch, dónde este decide en qué punto concreto de cada zona va a aparecer una pirámide (aleatoriamente), para posteriormente mandarle toda esta información al escenario. Para que este haga spawn de las pirámides con las funciones \"CreateObject\" y \"PlaceObject\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de rendimiento\n",
    "\n",
    "<img src=\"images/TensorBoard.png\"/>\n",
    "\n",
    "En base al análisis realizado de las gráficas proporcionadas por TensorBoard, y en vista al agente entrenado en acción, en nuestro ejemplo base, hemos obtenido las siguientes conclusiones en cuanto a cuáles son los parámetros que consideramos clave para:\n",
    "\n",
    "#### a) Para realizar el trabajo con éxito\n",
    "- El parámetro 'max steps', que incrementa las iteraciones/duración de una ejecución de entrenamiento. Este provoca que, cuanto más tiempo le demos, más probabilidades de completar la meta y, por lo tanto, de aprender con éxito.\n",
    "\n",
    "#### b) Para producir resultados smooth\n",
    "- El parámetro de recompensa, el cual indica que va por buen camino. Mediante una función de puntuación baja, obtendremos una reducción de pruebas de acciones exploradas, con tal de reducir la aleatoriedad para alcanzar la meta del agente a entrenar. Evitando así algunos altibajos o grandes picos en los resultados del proceso.\n",
    "\n",
    "#### c) Para entrenar más rápido\n",
    "- Los max steps, explicados anteriormente. Cuanto más sean, más probabilidades de completar la meta en una pasada, y evitar que se quede a punto de conseguirla. De manera que, aunque tarde más en una ejecución, recibirá esa recompensa antes para ir mejorando en menos entrenamientos, mejorando más rápido. Y en los pasos o ejecuciones menos difíciles, ya sean debido a la dificultad del entorno, no se verán afectados en gran manera, al terminar la ejecución tras obtener el goal.\n",
    "- Los rewards. Añadir recompensas intermedias son otra manera para que el agente entrene más rápido. Dividiendo el entrenamiento en fases con mini-goals, para obtener como checkpoints, y poder tener un feedback de que va por buen camino, agilizando el proceso.\n",
    "\n",
    "En cuanto a parámetros que no han funcionado, no los hemos encontrado. Han habido algunos irrelevantes pero que no han perjudicado. Mientras que los mencionados, que han funcionado, han sido los que han tenido una repercusión en el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Propuesta de nuevo caso\n",
    "\n",
    "En cuanto a la propuesta de un nuevo caso, hemos decidido que el agente haga el recorrido a la inversa del ejemplo de las pirámides. De manera que, lo primero que debe de hacer es aprender a discernir cual es la pirámide que debe derribar para coger el cubo que tiene encima y poder activar el interruptor. Este proceso inverso es más complicado para el agente que el original, debido a que el primer paso a realizar es más difícil de hacer que en el otro caso. Pues es más sencillo chocar con un botón aleatoriamente, que distinguir las pirámides para posteriormente derribar la pirámide objetivo y tocar el cubo de encima de esta. Este hecho ha necesitado que le demos al agente recompensas en varios puntos intermedios, sobre todo al obtener la pieza de la pirámide, para que se entrene de forma correcta y más sencilla, para que sea posible de realizarlo y/o no tarde tanto. Ya que con el modelo base de las recompensas, al agente le costaba muchísimo progresar a la inversa. El resto de ajustes y pasos es el mismo que en el ejemplo base.\n",
    "\n",
    "Así que, el principal cambio que ha recibido el nuevo método de entrenamiento ha sido recibir recompensas por pequeñas progresiones. Como por ejemplo, cuando consigue tocar el cubo de encima de la pirámide, el cual activa el interruptor final, además de tener que modificar el escenario y los códigos para que las pirámides hagan spawn al principio y no una vez pulsado el botón, como era el caso en el ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Team\n",
    "\n",
    "| <center> Samuel Balcells | <center> Sergi González |\n",
    "|-----------------|----------------|\n",
    "| <img src=\"images/samuel.jpg\"/> | <img src=\"images/sergi.png\"/> |\n",
    "| <center> samuelbalcellsvaldivia@enti.cat | <center> sergigonzalezppons@enti.cat |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
